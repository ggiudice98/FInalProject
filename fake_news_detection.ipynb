{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7hcFAPOZQw8+IngQ5wEhQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PQz6xsVkLmYx"},"outputs":[],"source":["# Dependencies and Installation\n","import os\n","# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.5.0'\n","spark_version = 'spark-3.5.0'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()\n","\n","import pandas as pd\n","import numpy as np\n","import nltk, string,re\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from sklearn import metrics\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import when, lit, col, to_date, rand, lower\n","\n","from sklearn import model_selection\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"]}]}